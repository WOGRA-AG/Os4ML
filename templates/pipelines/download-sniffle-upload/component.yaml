apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: download-sniffle-upload-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
    pipelines.kubeflow.org/pipeline_compilation_time: '2022-03-15T00:11:08.798692'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "os4ml", "name":
      "bucket", "optional": true, "type": "String"}, {"default": "titanic.xlsx", "name":
      "file_name", "optional": true, "type": "String"}, {"default": "10", "name":
      "max_categories", "optional": true, "type": "Integer"}, {"default": "databag_config.json",
      "name": "upload_file_name", "optional": true, "type": "String"}, {"default":
      "", "name": "pipeline-root"}, {"default": "pipeline/download-sniffle-upload",
      "name": "pipeline-name"}], "name": "download-sniffle-upload"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
spec:
  entrypoint: download-sniffle-upload
  templates:
  - name: download-from-objectstore
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'xlrd>=2.0.1' 'openpyxl>=3.0.9' 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def download_from_objectstore(bucket: str, file_name: str) -> Dataset:
            import pandas as pd
            data_uri: str = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/{bucket}/object/{file_name}'
            return pd.read_excel(data_uri, sheet_name='train').to_csv(index=False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - download_from_objectstore
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, download-from-objectstore,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        'file_name={{inputs.parameters.file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9.10-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {}, "outputArtifacts": {"Output": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/Output/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9.10-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      artifacts:
      - {name: download-from-objectstore-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "5b8d2e861c46f6e325d8928be48c6044c2dcdef346076a2d050ca19002337012",
          "url": "../../components/download-dataset-from-objectstore/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "file_name": "{{inputs.parameters.file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: download-sniffle-upload
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: max_categories}
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: upload_file_name}
    dag:
      tasks:
      - name: download-from-objectstore
        template: download-from-objectstore
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: file_name, value: '{{inputs.parameters.file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: sniff-datatypes
        template: sniff-datatypes
        dependencies: [download-from-objectstore]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: file_name, value: '{{inputs.parameters.file_name}}'}
          - {name: max_categories, value: '{{inputs.parameters.max_categories}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: download-from-objectstore-Output, from: '{{tasks.download-from-objectstore.outputs.artifacts.download-from-objectstore-Output}}'}
      - name: upload-file-to-objectstore
        template: upload-file-to-objectstore
        dependencies: [sniff-datatypes]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          - {name: upload_file_name, value: '{{inputs.parameters.upload_file_name}}'}
          artifacts:
          - {name: sniff-datatypes-Output, from: '{{tasks.sniff-datatypes.outputs.artifacts.sniff-datatypes-Output}}'}
  - name: sniff-datatypes
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def sniff_datatypes(csv_file: Input[Dataset],
                            max_categories: int = 10,
                            dataset_type: str = 'local_file',
                            file_name: str = '',
                            bucket_name: str = '') -> Dataset:
            import pandas as pd
            import json
            from enum import Enum
            from dataclasses import dataclass

            class ColumnDataType(str, Enum):
                NUMERICAL = 'numerical'
                DATE = 'date'
                CATEGORY = 'category'
                TEXT = 'text'

            class ColumnUsage(str, Enum):
                LABEL = 'label'
                FEATURE = 'feature'

            @dataclass
            class Column:
                name: str
                type: str
                usage: str
                num_entries: int

            def sniff_column_datatypes(df: pd.DataFrame) -> list[Column]:
                columns_and_types = (
                    (name, *sniff_series(column))
                    for name, column in df.iteritems()
                )
                *columns, last_column = columns_and_types
                feature_columns = (
                    Column(column, type_, ColumnUsage.FEATURE, num_entries)
                    for column, type_, num_entries in columns
                )
                label_column = Column(last_column[0], last_column[1],
                                      ColumnUsage.LABEL, last_column[2])
                return [*feature_columns, label_column]

            def sniff_series(series: pd.Series) -> tuple[ColumnDataType, int]:
                column_type = ColumnDataType.TEXT
                datatype = str(series.dtype)
                if 'int' in datatype or 'float' in datatype:
                    column_type = ColumnDataType.NUMERICAL
                if 'date' in datatype:
                    column_type = ColumnDataType.DATE
                if series.nunique() <= max_categories:
                    column_type = ColumnDataType.CATEGORY
                return column_type, series.size

            def get_num_rows(columns: list[Column]) -> int:
                if not columns:
                    return 0
                first, *rest = columns
                rows = first.num_entries
                assert (column.num_entries == rows for column in columns)
                return rows

            with open(csv_file.path, 'r') as input_file:
                df = pd.read_csv(input_file)

            column_info = sniff_column_datatypes(df)
            num_rows = get_num_rows(column_info)
            num_cols = len(column_info)
            column_info_dicts = [column.__dict__ for column in column_info]
            return json.dumps({
                'datasetType': dataset_type,
                'fileName': file_name,
                'databagName': file_name,
                'bucketName': bucket_name,
                'numberRows': num_rows,
                'numberColumns': num_cols,
                'columns': column_info_dicts
            })

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - sniff_datatypes
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, sniff-datatypes, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket_name={{inputs.parameters.bucket}}',
        dataset_type=local_file, 'file_name={{inputs.parameters.file_name}}', 'max_categories={{inputs.parameters.max_categories}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'amancevice/pandas:1.4.1-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket_name": {"type":
          "STRING"}, "dataset_type": {"type": "STRING"}, "file_name": {"type": "STRING"},
          "max_categories": {"type": "INT"}}, "inputArtifacts": {"csv_file": {"metadataPath":
          "/tmp/inputs/csv_file/data", "schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1"}}, "outputParameters": {}, "outputArtifacts":
          {"Output": {"schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/Output/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: amancevice/pandas:1.4.1-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: max_categories}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: download-from-objectstore-Output, path: /tmp/inputs/csv_file/data}
    outputs:
      artifacts:
      - {name: sniff-datatypes-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "fade873ee868218663e6f5f7534e4a41a99c949e49b02b26871c6cf3c54a48f4",
          "url": "../../components/sniffle-dataset/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket_name": "{{inputs.parameters.bucket}}",
          "dataset_type": "local_file", "file_name": "{{inputs.parameters.file_name}}",
          "max_categories": "{{inputs.parameters.max_categories}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: upload-file-to-objectstore
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def upload_file_to_objectstore(file: Input[Dataset],
                                       bucket: str,
                                       file_name: str):
            import requests
            url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1' \
                  f'/objectstore/{bucket}/object/{file_name}'
            with open(file.path, 'rb') as payload:
                response = requests.put(url, data=payload)
            assert response.status_code == 200

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - upload_file_to_objectstore
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, upload-file-to-objectstore,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        'file_name={{inputs.parameters.upload_file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {"file":
          {"metadataPath": "/tmp/inputs/file/data", "schemaTitle": "system.Dataset",
          "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters": {},
          "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: upload_file_name}
      artifacts:
      - {name: sniff-datatypes-Output, path: /tmp/inputs/file/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "6551f1e1499490a963ed50698fcc4e9cf48d7d6f370861d97579355c288133d7",
          "url": "../../components/upload_file_to_objectstore/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "file_name": "{{inputs.parameters.upload_file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: bucket, value: os4ml}
    - {name: file_name, value: titanic.xlsx}
    - {name: max_categories, value: '10'}
    - {name: upload_file_name, value: databag_config.json}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/download-sniffle-upload}
  serviceAccountName: pipeline-runner
