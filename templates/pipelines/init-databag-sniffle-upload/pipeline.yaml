apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: init-databag-sniffle-upload-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
    pipelines.kubeflow.org/pipeline_compilation_time: '2022-05-11T17:16:58.187447'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "os4ml", "name":
      "bucket", "optional": true, "type": "String"}, {"default": "titanic.xlsx", "name":
      "file_name", "optional": true, "type": "String"}, {"default": "", "name": "solution_name",
      "optional": true, "type": "String"}, {"default": "10", "name": "max_categories",
      "optional": true, "type": "Integer"}, {"default": "", "name": "pipeline-root"},
      {"default": "pipeline/init-databag-sniffle-upload", "name": "pipeline-name"}],
      "name": "init-databag-sniffle-upload"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
spec:
  entrypoint: init-databag-sniffle-upload
  templates:
  - name: create-databag
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def create_databag(file: Input[Dataset], bucket: str):
            import requests
            import json
            url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1' \
                  f'/objectstore/databag/{bucket}'
            with open(file.path) as json_file:
                json_data = json.load(json_file)
                print(json_data)
                requests.put(url, json=json_data)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - create_databag
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, create-databag, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}}, "inputArtifacts": {"file": {"metadataPath": "/tmp/inputs/file/data",
          "schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: sniff-datatypes-Output, path: /tmp/inputs/file/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "147201a8aba636b3d23406ea828ca2a20a9a7d016eeb5a652c4fbe25a6a7479e",
          "url": "../../components/create-databag/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: init-databag
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'xlrd>=2.0.1' 'openpyxl>=3.0.9' 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def init_databag(
            bucket: str, file_name: str
        ) -> NamedTuple("DatabagInfo", [("databag_type", str), ("dataset", Dataset),]):
            """
            Inits the databag by specifying its type and creating the dataset.
            If the file is a zip file it should only contain directories in the top level.
            The names of them are used as labels and the files they contain are used as features.
            """
            import enum
            import pathlib
            import tempfile
            import zipfile
            from collections.abc import Generator
            from typing import BinaryIO
            from urllib.parse import urlparse

            import pandas as pd
            import requests

            class DatabagTypes(str, enum.Enum):
                local_file = "local_file"
                zip_file = "zip_file"
                shepard_url = "shepard_url"

            def _is_uri(uri: str) -> bool:
                parsed = urlparse(uri)
                return True if parsed.scheme and parsed.netloc else False

            def _is_shepard_uri(uri: str) -> bool:
                if not _is_uri(uri):
                    return False
                if '/shepard/' in uri:
                    return True
                return False

            def _extract_filename_from_uri(file_url):
                parsed_url = urlparse(file_url)
                return pathlib.Path(parsed_url.path).name

            def download_file(url: str, output_file: BinaryIO, chunk_size=128) -> None:
                response = requests.get(url, stream=True)
                for chunk in response.iter_content(chunk_size=chunk_size):
                    output_file.write(chunk)

            def iter_dirs_of_zip_with_labels(
                zip_file: BinaryIO,
            ) -> Generator[tuple[str, str], None, None]:
                with zipfile.ZipFile(zip_file) as root:
                    unpacked_root_dir = next(zipfile.Path(root).iterdir())
                    for label_dir in unpacked_root_dir.iterdir():
                        label = label_dir.name
                        for file in label_dir.iterdir():
                            file_name = file.filename.resolve().relative_to(
                                unpacked_root_dir.parent.filename.resolve()
                            )
                            yield str(file_name), label

            file_name_is_uri = _is_uri(file_name)

            if file_name_is_uri:
                data_uri = file_name
                file_name = _extract_filename_from_uri(file_name)
            else:
                data_uri = f"http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/{bucket}/object/{file_name}"

            file_path = pathlib.Path(file_name)
            match file_path.suffix:
                case ".csv":
                    df = pd.read_csv(data_uri)
                    databag_type = DatabagTypes.local_file
                case ".xls" | ".xlsx" | ".xlsm" | ".xlsb" | ".odf" | ".ods":
                    df = pd.read_excel(data_uri, sheet_name=0)
                    databag_type = DatabagTypes.local_file
                case ".zip":
                    with tempfile.NamedTemporaryFile() as tmp_file:
                        download_file(data_uri, tmp_file)
                        df = pd.DataFrame(
                            iter_dirs_of_zip_with_labels(tmp_file),
                            columns=["file", "label"],
                        )
                    databag_type = DatabagTypes.zip_file
                case _:
                    raise NotImplementedError()

            databag_info = NamedTuple(
                "DatabagInfo",
                [
                    ("databag_type", str),
                    ("dataset", Dataset),
                ],
            )
            return databag_info(databag_type.value, df.to_csv(index=False))

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - init_databag
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, init-databag, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        'file_name={{inputs.parameters.file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {"databag_type": {"type": "STRING", "path": "/tmp/outputs/databag_type/data"}},
          "outputArtifacts": {"dataset": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/dataset/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      parameters:
      - name: init-databag-databag_type
        valueFrom: {path: /tmp/outputs/databag_type/data}
      artifacts:
      - {name: init-databag-databag_type, path: /tmp/outputs/databag_type/data}
      - {name: init-databag-dataset, path: /tmp/outputs/dataset/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "8e4e157bbb7fb8b445c0d49c03abb1e1319e8c9ef0025eec75020b8adea48b9b",
          "url": "../../components/init-databag/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "file_name": "{{inputs.parameters.file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: init-databag-sniffle-upload
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: max_categories}
      - {name: pipeline-name}
      - {name: pipeline-root}
    dag:
      tasks:
      - name: create-databag
        template: create-databag
        dependencies: [sniff-datatypes]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: sniff-datatypes-Output, from: '{{tasks.sniff-datatypes.outputs.artifacts.sniff-datatypes-Output}}'}
      - name: init-databag
        template: init-databag
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: file_name, value: '{{inputs.parameters.file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: sniff-datatypes
        template: sniff-datatypes
        dependencies: [init-databag]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: file_name, value: '{{inputs.parameters.file_name}}'}
          - {name: init-databag-databag_type, value: '{{tasks.init-databag.outputs.parameters.init-databag-databag_type}}'}
          - {name: max_categories, value: '{{inputs.parameters.max_categories}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: init-databag-dataset, from: '{{tasks.init-databag.outputs.artifacts.init-databag-dataset}}'}
  - name: sniff-datatypes
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def sniff_datatypes(
            dataset: Input[Dataset],
            dataset_type: str = "local_file",
            max_categories: int = 10,
            file_name: str = "",
            bucket_name: str = "",
        ) -> Dataset:
            """
            Guesses the datatypes of the columns in the dataframe.
            For local_file databags the type is derived from the values in the dataframe.
            For zip_file databags the type is derived from the suffix of the file names in the dataframe.
            """
            import dataclasses
            import enum
            import json
            import pathlib

            import pandas as pd

            from urllib.parse import urlparse

            class ColumnDataType(str, enum.Enum):
                NUMERICAL = "numerical"
                DATE = "date"
                CATEGORY = "category"
                TEXT = "text"
                IMAGE = "image"

            class ColumnUsage(str, enum.Enum):
                LABEL = "label"
                FEATURE = "feature"

            @dataclasses.dataclass
            class Column:
                name: str
                type: str
                usage: str
                num_entries: int

            def sniff_column_datatypes(df: pd.DataFrame) -> list[Column]:
                columns_and_types = (
                    (name, *sniff_series(column)) for name, column in df.iteritems()
                )
                *columns, last_column = columns_and_types
                feature_columns = (
                    Column(column, type_, ColumnUsage.FEATURE, num_entries)
                    for column, type_, num_entries in columns
                )
                label_column = Column(
                    last_column[0], last_column[1], ColumnUsage.LABEL, last_column[2]
                )
                return [*feature_columns, label_column]

            def sniff_series(series: pd.Series) -> tuple[ColumnDataType, int]:
                column_type = ColumnDataType.TEXT
                datatype = str(series.dtype)
                if "int" in datatype or "float" in datatype:
                    column_type = ColumnDataType.NUMERICAL
                if "date" in datatype:
                    column_type = ColumnDataType.DATE
                if series.nunique() <= max_categories:
                    column_type = ColumnDataType.CATEGORY
                return column_type, series.size

            def sniff_zip_types(df: pd.DataFrame) -> list[Column]:
                example_file = df["file"][0]
                num_files = len(df["file"])
                suffix = pathlib.Path(example_file).suffix.lower()
                if suffix in (".jpg", ".jpeg", ".png", ".tiff"):
                    file_column = Column(
                        "file", ColumnDataType.IMAGE, ColumnUsage.FEATURE, num_files
                    )
                else:
                    raise NotImplementedError()
                num_labels = len(df["label"])
                label_column = Column(
                    "label", ColumnDataType.CATEGORY, ColumnUsage.LABEL, num_labels
                )
                return [file_column, label_column]

            def get_num_rows(columns: list[Column]) -> int:
                if not columns:
                    return 0
                first, *rest = columns
                rows = first.num_entries
                assert (column.num_entries == rows for column in columns)
                return rows

            def _is_uri(uri: str) -> bool:
                parsed = urlparse(uri)
                return True if parsed.scheme and parsed.netloc else False

            def _extract_filename_from_uri(file_url):
                parsed_url = urlparse(file_url)
                return pathlib.Path(parsed_url.path).name

            with open(dataset.path, "r") as dataset_file:
                df = pd.read_csv(dataset_file)

            if dataset_type == "local_file":
                column_info = sniff_column_datatypes(df)
            elif dataset_type == "zip_file":
                column_info = sniff_zip_types(df)
            else:
                raise NotImplementedError()

            databag_name = _extract_filename_from_uri(file_name) if _is_uri(file_name) else file_name
            num_rows = get_num_rows(column_info)
            num_cols = len(column_info)
            column_info_dicts = [column.__dict__ for column in column_info]
            return json.dumps(
                {
                    "dataset_type": dataset_type,
                    "file_name": file_name,
                    "databag_name": databag_name,
                    "bucket_name": bucket_name,
                    "number_rows": num_rows,
                    "number_columns": num_cols,
                    "columns": column_info_dicts,
                }
            )

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - sniff_datatypes
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, sniff-datatypes, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket_name={{inputs.parameters.bucket}}',
        'dataset_type={{inputs.parameters.init-databag-databag_type}}', 'file_name={{inputs.parameters.file_name}}',
        'max_categories={{inputs.parameters.max_categories}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'amancevice/pandas:1.4.1-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket_name": {"type":
          "STRING"}, "dataset_type": {"type": "STRING"}, "file_name": {"type": "STRING"},
          "max_categories": {"type": "INT"}}, "inputArtifacts": {"dataset": {"metadataPath":
          "/tmp/inputs/dataset/data", "schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1"}}, "outputParameters": {}, "outputArtifacts":
          {"Output": {"schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/Output/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: amancevice/pandas:1.4.1-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: init-databag-databag_type}
      - {name: max_categories}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: init-databag-dataset, path: /tmp/inputs/dataset/data}
    outputs:
      artifacts:
      - {name: sniff-datatypes-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "c3f860facdb5a134a6fc271b836bfdf267a25ae9f50ef6a16515eb7356a12ec2",
          "url": "../../components/sniffle-dataset/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket_name": "{{inputs.parameters.bucket}}",
          "dataset_type": "{{inputs.parameters.init-databag-databag_type}}", "file_name":
          "{{inputs.parameters.file_name}}", "max_categories": "{{inputs.parameters.max_categories}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: bucket, value: os4ml}
    - {name: file_name, value: titanic.xlsx}
    - {name: solution_name, value: ''}
    - {name: max_categories, value: '10'}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/init-databag-sniffle-upload}
  serviceAccountName: pipeline-runner
