apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: download-and-sniff-and-upload-pipeline-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
    pipelines.kubeflow.org/pipeline_compilation_time: '2022-02-24T15:30:35.827057'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "os4ml", "name":
      "download_bucket", "optional": true, "type": "String"}, {"default": "titanic.xlsx",
      "name": "download_file_name", "optional": true, "type": "String"}, {"default":
      "10", "name": "max_categories", "optional": true, "type": "Integer"}, {"default":
      "os4ml", "name": "upload_bucket", "optional": true, "type": "String"}, {"default":
      "settings.json", "name": "upload_file_name", "optional": true, "type": "String"},
      {"default": "", "name": "pipeline-root"}, {"default": "pipeline/download-and-sniff-and-upload-pipeline",
      "name": "pipeline-name"}], "name": "download-and-sniff-and-upload-pipeline"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
spec:
  entrypoint: download-and-sniff-and-upload-pipeline
  templates:
  - name: download-and-sniff-and-upload-pipeline
    inputs:
      parameters:
      - {name: download_bucket}
      - {name: download_file_name}
      - {name: max_categories}
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: upload_bucket}
      - {name: upload_file_name}
    dag:
      tasks:
      - name: download-from-objectstore
        template: download-from-objectstore
        arguments:
          parameters:
          - {name: download_bucket, value: '{{inputs.parameters.download_bucket}}'}
          - {name: download_file_name, value: '{{inputs.parameters.download_file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: sniff-datatypes
        template: sniff-datatypes
        dependencies: [download-from-objectstore]
        arguments:
          parameters:
          - {name: max_categories, value: '{{inputs.parameters.max_categories}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: download-from-objectstore-Output, from: '{{tasks.download-from-objectstore.outputs.artifacts.download-from-objectstore-Output}}'}
      - name: upload-file-to-objectstore
        template: upload-file-to-objectstore
        dependencies: [sniff-datatypes]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          - {name: upload_bucket, value: '{{inputs.parameters.upload_bucket}}'}
          - {name: upload_file_name, value: '{{inputs.parameters.upload_file_name}}'}
          artifacts:
          - {name: sniff-datatypes-Output, from: '{{tasks.sniff-datatypes.outputs.artifacts.sniff-datatypes-Output}}'}
  - name: download-from-objectstore
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'xlrd>=2.0.1' 'openpyxl>=3.0.9' 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def download_from_objectstore(bucket: str, file_name: str) -> Dataset:
            import pandas as pd
            data_uri: str = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/{bucket}/object/{file_name}'
            return pd.read_excel(data_uri, sheet_name='train').to_csv(index=False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - download_from_objectstore
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, download-from-objectstore,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.download_bucket}}',
        'file_name={{inputs.parameters.download_file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9.10-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {}, "outputArtifacts": {"Output": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/Output/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9.10-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: download_bucket}
      - {name: download_file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      artifacts:
      - {name: download-from-objectstore-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "5b8d2e861c46f6e325d8928be48c6044c2dcdef346076a2d050ca19002337012",
          "url": "../../components/download-dataset-from-objectstore/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.download_bucket}}",
          "file_name": "{{inputs.parameters.download_file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: sniff-datatypes
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def sniff_datatypes(csv_file: Input[Dataset],
                            max_categories: int = 10) -> Dataset:
            import pandas as pd
            import json
            from enum import Enum

            class ColumnDataType(str, Enum):
                NUMERICAL = 'numerical'
                DATE = 'date'
                CATEGORY = 'category'
                TEXT = 'text'

            class ColumnUsage(str, Enum):
                LABEL = 'label'
                FEATURE = 'feature'

            def sniff_column_datatypes(df: pd.DataFrame):
                columns_and_types = (
                    (name, sniff_series(column))
                    for name, column in df.iteritems()
                )
                *columns, last_column = columns_and_types
                feature_columns = (
                    create_entry(column, type_, ColumnUsage.FEATURE)
                    for column, type_ in columns
                )
                label_column = create_entry(*last_column, ColumnUsage.LABEL)
                return [*feature_columns, label_column]

            def create_entry(name, type_, usage):
                return {
                    'name': name,
                    'type': type_,
                    'usage': usage,
                }

            def sniff_series(series: pd.Series) -> ColumnDataType:
                column_type = ColumnDataType.TEXT
                datatype = str(series.dtype)
                if 'int' in datatype or 'float' in datatype:
                    column_type = ColumnDataType.NUMERICAL
                if 'date' in datatype:
                    column_type = ColumnDataType.DATE
                if series.nunique() <= max_categories:
                    column_type = ColumnDataType.CATEGORY
                return column_type

            with open(csv_file.path, 'r') as input_file:
                df = pd.read_csv(input_file)

            column_info = sniff_column_datatypes(df)
            return json.dumps(column_info)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - sniff_datatypes
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, sniff-datatypes, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'max_categories={{inputs.parameters.max_categories}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'amancevice/pandas:1.4.1-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"max_categories":
          {"type": "INT"}}, "inputArtifacts": {"csv_file": {"metadataPath": "/tmp/inputs/csv_file/data",
          "schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"Output": {"schemaTitle":
          "system.Dataset", "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath":
          "/tmp/outputs/Output/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: amancevice/pandas:1.4.1-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: max_categories}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: download-from-objectstore-Output, path: /tmp/inputs/csv_file/data}
    outputs:
      artifacts:
      - {name: sniff-datatypes-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "eae51b49aad4ba2d51bae935108a89a19b5da80f86a2b7fd104757979de459c8",
          "url": "../../components/sniff-datatypes/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"max_categories": "{{inputs.parameters.max_categories}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: upload-file-to-objectstore
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def upload_file_to_objectstore(file: Input[Dataset],
                                       bucket: str,
                                       file_name: str):
            import requests
            url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1' \
                  f'/objectstore/{bucket}/object/{file_name}'
            with open(file.path, 'rb') as payload:
                response = requests.put(url, data=payload)
            assert response.status_code == 200

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - upload_file_to_objectstore
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, upload-file-to-objectstore,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.upload_bucket}}',
        'file_name={{inputs.parameters.upload_file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {"file":
          {"metadataPath": "/tmp/inputs/file/data", "schemaTitle": "system.Dataset",
          "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters": {},
          "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: upload_bucket}
      - {name: upload_file_name}
      artifacts:
      - {name: sniff-datatypes-Output, path: /tmp/inputs/file/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "6551f1e1499490a963ed50698fcc4e9cf48d7d6f370861d97579355c288133d7",
          "url": "../../components/upload_file_to_objectstore/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.upload_bucket}}",
          "file_name": "{{inputs.parameters.upload_file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: download_bucket, value: os4ml}
    - {name: download_file_name, value: titanic.xlsx}
    - {name: max_categories, value: '10'}
    - {name: upload_bucket, value: os4ml}
    - {name: upload_file_name, value: settings.json}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/download-and-sniff-and-upload-pipeline}
  serviceAccountName: pipeline-runner
