apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: titanic-randomforest-pipeline-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
    pipelines.kubeflow.org/pipeline_compilation_time: '2022-05-13T12:17:57.994518'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "os4ml", "name":
      "bucket", "optional": true, "type": "String"}, {"default": "titanic.xlsx", "name":
      "file_name", "optional": true, "type": "String"}, {"default": "", "name": "solution_name",
      "optional": true, "type": "String"}, {"default": "", "name": "pipeline-root"},
      {"default": "pipeline/titanic-randomforest-pipeline", "name": "pipeline-name"}],
      "name": "titanic-randomforest-pipeline"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
spec:
  entrypoint: titanic-randomforest-pipeline
  templates:
  - name: get-metrics
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def get_metrics(metrics: Input[Metrics], solution_name: str = ""):
            import requests
            from datetime import datetime

            if "accuracy" in metrics.metadata:
                accuracy = metrics.metadata["accuracy"]
                url = f"http://os4ml-jobmanager.os4ml:8000/apis/v1beta1/jobmanager/solution/{solution_name}"

                solution = requests.get(url).json()
                if "metrics" not in solution or solution["metrics"] is None:
                    solution["metrics"] = dict()
                solution["metrics"]["accuracy"] = accuracy
                solution["completionTime"] = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')

                requests.put(url, json=solution)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - get_metrics
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, get-metrics, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'solution_name={{inputs.parameters.solution_name}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"solution_name":
          {"type": "STRING"}}, "inputArtifacts": {"metrics": {"metadataPath": "/tmp/inputs/metrics/data",
          "schemaTitle": "system.Metrics", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: solution_name}
      artifacts:
      - {name: train-random-forest-metrics, path: /tmp/inputs/metrics/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "e5612aa4629dc6d0806fb0e86f32fc26f43d8cb125780d6a37917bda6b73f47d",
          "url": "../../components/get-metrics/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"solution_name": "{{inputs.parameters.solution_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: init-databag
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'xlrd>=2.0.1' 'openpyxl>=3.0.9' 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def init_databag(
            bucket: str, file_name: str
        ) -> NamedTuple("DatabagInfo", [("databag_type", str), ("dataset", Dataset),]):
            """
            Inits the databag by specifying its type and creating the dataset.
            If the file is a zip file it should only contain directories in the top level.
            The names of them are used as labels and the files they contain are used as features.
            """
            import enum
            import pathlib
            import tempfile
            import zipfile
            from collections.abc import Generator
            from typing import BinaryIO
            from urllib.parse import urlparse

            import pandas as pd
            import requests

            class DatabagTypes(str, enum.Enum):
                local_file = "local_file"
                zip_file = "zip_file"
                file_url = "file_url"
                shepard_url = "shepard_url"

            def _is_uri(uri: str) -> bool:
                parsed = urlparse(uri)
                return True if parsed.scheme and parsed.netloc else False

            def _is_shepard_uri(uri: str) -> bool:
                if not _is_uri(uri):
                    return False
                return '/shepard/api/' in uri

            def _extract_filename_from_uri(file_url):
                parsed_url = urlparse(file_url)
                return pathlib.Path(parsed_url.path).name

            def download_file(url: str, output_file: BinaryIO, chunk_size=128) -> None:
                response = requests.get(url, stream=True)
                for chunk in response.iter_content(chunk_size=chunk_size):
                    output_file.write(chunk)

            def iter_dirs_of_zip_with_labels(
                zip_file: BinaryIO,
            ) -> Generator[tuple[str, str], None, None]:
                with zipfile.ZipFile(zip_file) as root:
                    unpacked_root_dir = next(zipfile.Path(root).iterdir())
                    for label_dir in unpacked_root_dir.iterdir():
                        label = label_dir.name
                        for file in label_dir.iterdir():
                            file_name = file.filename.resolve().relative_to(
                                unpacked_root_dir.parent.filename.resolve()
                            )
                            yield str(file_name), label

            databag_type = None
            databag_info = NamedTuple(
                "DatabagInfo",
                [
                    ("databag_type", str),
                    ("dataset", Dataset),
                ],
            )

            if _is_shepard_uri(file_name):
                databag_type = DatabagTypes.shepard_url
                df = pd.DataFrame([1], columns=['a'])
                return databag_info(databag_type.value, df.to_csv(index=False))
            elif _is_uri(file_name):
                data_uri = file_name
                file_name = _extract_filename_from_uri(file_name)
                databag_type = DatabagTypes.file_url
            else:
                data_uri = f"http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/{bucket}/object/{file_name}"

            file_path = pathlib.Path(file_name)
            match file_path.suffix:
                case ".csv":
                    df = pd.read_csv(data_uri)
                    databag_type = databag_type or DatabagTypes.local_file
                case ".xls" | ".xlsx" | ".xlsm" | ".xlsb" | ".odf" | ".ods":
                    df = pd.read_excel(data_uri, sheet_name=0)
                    databag_type = databag_type or DatabagTypes.local_file
                case ".zip":
                    with tempfile.NamedTemporaryFile() as tmp_file:
                        download_file(data_uri, tmp_file)
                        df = pd.DataFrame(
                            iter_dirs_of_zip_with_labels(tmp_file),
                            columns=["file", "label"],
                        )
                    databag_type = DatabagTypes.zip_file
                case _:
                    raise NotImplementedError()

            return databag_info(databag_type.value, df.to_csv(index=False))

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - init_databag
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, init-databag, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        'file_name={{inputs.parameters.file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {"databag_type": {"type": "STRING", "path": "/tmp/outputs/databag_type/data"}},
          "outputArtifacts": {"dataset": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/dataset/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      artifacts:
      - {name: init-databag-databag_type, path: /tmp/outputs/databag_type/data}
      - {name: init-databag-dataset, path: /tmp/outputs/dataset/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "f45d968b1e58757c78b0b7c54e4a34baadd6f4e98f1bb35c3073ca882526dd48",
          "url": "../../components/init-databag/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "file_name": "{{inputs.parameters.file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: preprocess-data
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def preprocess_data(dataframe: Input[Dataset]) -> NamedTuple("Data",
                                                                     [('x', Dataset),
                                                                      ('y', Dataset)]):
            import pandas as pd
            import re
            import numpy as np
            from collections import namedtuple

            with open(dataframe.path, 'r') as f:
                df = pd.read_csv(f)

            # example code from https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8
            df = df.drop(["PassengerId"], axis=1)
            deck = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "U": 8}
            df["Cabin"] = df["Cabin"].fillna("U0")
            df["Deck"] = df["Cabin"].map(
                lambda x: re.compile("([a-zA-Z]+)").search(x).group())
            df["Deck"] = df["Deck"].map(deck).fillna(0).astype(int)
            df = df.drop(["Cabin"], axis=1)

            mean = df["Age"].mean()
            std = df["Age"].std()
            is_null = df["Age"].isnull().sum()
            # compute random numbers between the mean, std and is_null
            rand_age = np.random.randint(mean - std, mean + std, size=is_null)
            age_slice = df["Age"].copy()
            age_slice[np.isnan(age_slice)] = rand_age
            df["Age"] = age_slice.astype(int)

            common_value = 'S'
            df['Embarked'] = df['Embarked'].fillna(common_value)

            df["Fare"] = df["Fare"].fillna(0).astype(int)

            titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}

            # extract titles
            df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
            # replace titles with a more common title or as Rare
            df['Title'] = df['Title'].replace(
                ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr',
                 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
            df['Title'] = df['Title'].replace('Mlle', 'Miss')
            df['Title'] = df['Title'].replace('Ms', 'Miss')
            df['Title'] = df['Title'].replace('Mme', 'Mrs')
            # convert titles into numbers
            df['Title'] = df['Title'].map(titles)
            # filling NaN with 0, to get safe
            df['Title'] = df['Title'].fillna(0)

            df = df.drop(['Name'], axis=1)

            genders = {"male": 0, "female": 1}
            df["Sex"] = df["Sex"].map(genders)

            df = df.drop(["Ticket"], axis=1)

            ports = {"S": 0, "C": 1, "Q": 2}
            df["Embarked"] = df["Embarked"].map(ports)

            df['relatives'] = df['SibSp'] + df['Parch']

            # Create Categories
            df['Age'] = df['Age'].astype(int)
            df.loc[df['Age'] <= 11, 'Age'] = 0
            df.loc[(df['Age'] > 11) & (df['Age'] <= 18), 'Age'] = 1
            df.loc[(df['Age'] > 18) & (df['Age'] <= 22), 'Age'] = 2
            df.loc[(df['Age'] > 22) & (df['Age'] <= 27), 'Age'] = 3
            df.loc[(df['Age'] > 27) & (df['Age'] <= 33), 'Age'] = 4
            df.loc[(df['Age'] > 33) & (df['Age'] <= 40), 'Age'] = 5
            df.loc[(df['Age'] > 40) & (df['Age'] <= 66), 'Age'] = 6
            df.loc[df['Age'] > 66, 'Age'] = 6

            # Fare
            df.loc[df['Fare'] <= 7.91, 'Fare'] = 0
            df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1
            df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare'] = 2
            df.loc[(df['Fare'] > 31) & (df['Fare'] <= 99), 'Fare'] = 3
            df.loc[(df['Fare'] > 99) & (df['Fare'] <= 250), 'Fare'] = 4
            df.loc[df['Fare'] > 250, 'Fare'] = 5
            df['Fare'] = df['Fare'].astype(int)

            df['Age_Class'] = df['Age'] * df['Pclass']

            df['Fare_Per_Person'] = df['Fare'] / (df['relatives'] + 1)
            df['Fare_Per_Person'] = df['Fare_Per_Person'].astype(int)

            df_x = df.drop(["Survived"], axis=1)
            df_y = df["Survived"]

            output = namedtuple("Data", ["x", "y"])
            return output(df_x.to_csv(index=False), df_y.to_csv(index=False))

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - preprocess_data
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, preprocess-data, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9.10-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"dataframe": {"metadataPath": "/tmp/inputs/dataframe/data", "schemaTitle":
          "system.Dataset", "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters":
          {}, "outputArtifacts": {"x": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/x/data"}, "y":
          {"schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/y/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9.10-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: init-databag-dataset, path: /tmp/inputs/dataframe/data}
    outputs:
      artifacts:
      - {name: preprocess-data-x, path: /tmp/outputs/x/data}
      - {name: preprocess-data-y, path: /tmp/outputs/y/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "24c4814498ab4805779867d20206c01f58be1be5895b9a27a7728340b7007f83",
          "url": "../../components/preprocess-data/component.yaml"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: titanic-randomforest-pipeline
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: solution_name}
    dag:
      tasks:
      - name: get-metrics
        template: get-metrics
        dependencies: [train-random-forest]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          - {name: solution_name, value: '{{inputs.parameters.solution_name}}'}
          artifacts:
          - {name: train-random-forest-metrics, from: '{{tasks.train-random-forest.outputs.artifacts.train-random-forest-metrics}}'}
      - name: init-databag
        template: init-databag
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: file_name, value: '{{inputs.parameters.file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: preprocess-data
        template: preprocess-data
        dependencies: [init-databag]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: init-databag-dataset, from: '{{tasks.init-databag.outputs.artifacts.init-databag-dataset}}'}
      - name: train-random-forest
        template: train-random-forest
        dependencies: [preprocess-data]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: preprocess-data-x, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-x}}'}
          - {name: preprocess-data-y, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-y}}'}
      - name: update-status
        template: update-status
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          - {name: solution_name, value: '{{inputs.parameters.solution_name}}'}
      - name: update-status-2
        template: update-status-2
        dependencies: [init-databag]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          - {name: solution_name, value: '{{inputs.parameters.solution_name}}'}
          artifacts:
          - {name: init-databag-dataset, from: '{{tasks.init-databag.outputs.artifacts.init-databag-dataset}}'}
      - name: update-status-3
        template: update-status-3
        dependencies: [train-random-forest]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          - {name: solution_name, value: '{{inputs.parameters.solution_name}}'}
          artifacts:
          - {name: train-random-forest-metrics, from: '{{tasks.train-random-forest.outputs.artifacts.train-random-forest-metrics}}'}
  - name: train-random-forest
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'scikit-learn>=1.0.2' 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def train_random_forest(
            data_x: Input[Dataset],
            data_y: Input[Dataset],
            class_metrics: Output[ClassificationMetrics],
            metrics: Output[Metrics],
        ) -> float:
            import pandas as pd
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import confusion_matrix
            from sklearn.model_selection import train_test_split

            random_state = 42
            with open(data_x.path, "r") as f:
                df_x = pd.read_csv(f)
            with open(data_y.path, "r") as f:
                df_y = pd.read_csv(f)
            x_train, x_test, y_train, y_test = train_test_split(
                df_x, df_y, test_size=0.33, random_state=random_state
            )
            clf = RandomForestClassifier(n_estimators=100, random_state=random_state)
            clf.fit(x_train, y_train)
            score = clf.score(x_test, y_test)
            y_pred = clf.predict(x_test)
            conf_matrix = confusion_matrix(y_test, y_pred).tolist()
            class_metrics.log_confusion_matrix(["Died", "Survived"], conf_matrix)
            metrics.log_metric("accuracy", score)
            return score

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - train_random_forest
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, train-random-forest, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9.10-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"data_x": {"metadataPath": "/tmp/inputs/data_x/data", "schemaTitle": "system.Dataset",
          "instanceSchema": "", "schemaVersion": "0.0.1"}, "data_y": {"metadataPath":
          "/tmp/inputs/data_y/data", "schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1"}}, "outputParameters": {"Output": {"type":
          "DOUBLE", "path": "/tmp/outputs/Output/data"}}, "outputArtifacts": {"class_metrics":
          {"schemaTitle": "system.ClassificationMetrics", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/class_metrics/data"}, "metrics":
          {"schemaTitle": "system.Metrics", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9.10-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: preprocess-data-x, path: /tmp/inputs/data_x/data}
      - {name: preprocess-data-y, path: /tmp/inputs/data_y/data}
    outputs:
      artifacts:
      - {name: train-random-forest-Output, path: /tmp/outputs/Output/data}
      - {name: train-random-forest-class_metrics, path: /tmp/outputs/class_metrics/data}
      - {name: train-random-forest-metrics, path: /tmp/outputs/metrics/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "057f2834076ed656a12a1f505356549d9cfb62f8178a0846320ad986f3dd4430",
          "url": "../../components/train-random-forest/component.yaml"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: update-status
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def update_status(solution_name: str = "", status: str = "", depends_on: Artifact = None):
            import requests

            url = f"http://os4ml-jobmanager.os4ml:8000/apis/v1beta1/jobmanager/solution/{solution_name}"

            solution = requests.get(url).json()
            solution["status"] = status

            requests.put(url, json=solution)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - update_status
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, update-status, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'solution_name={{inputs.parameters.solution_name}}',
        status=Solution created, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"solution_name":
          {"type": "STRING"}, "status": {"type": "STRING"}}, "inputArtifacts": {},
          "outputParameters": {}, "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: solution_name}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "d792d5dfcb1825d1452f5784a0d8e487d2c7d8c2d6dd5bf53e67d7a2c0b59406",
          "url": "../../components/update-status/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"solution_name": "{{inputs.parameters.solution_name}}",
          "status": "Solution created"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: update-status-2
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def update_status(solution_name: str = "", status: str = "", depends_on: Artifact = None):
            import requests

            url = f"http://os4ml-jobmanager.os4ml:8000/apis/v1beta1/jobmanager/solution/{solution_name}"

            solution = requests.get(url).json()
            solution["status"] = status

            requests.put(url, json=solution)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - update_status
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, update-status-2, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'solution_name={{inputs.parameters.solution_name}}',
        status=Solver running, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"solution_name":
          {"type": "STRING"}, "status": {"type": "STRING"}}, "inputArtifacts": {"depends_on":
          {"metadataPath": "/tmp/inputs/depends_on/data", "schemaTitle": "system.Artifact",
          "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters": {},
          "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: solution_name}
      artifacts:
      - {name: init-databag-dataset, path: /tmp/inputs/depends_on/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "d792d5dfcb1825d1452f5784a0d8e487d2c7d8c2d6dd5bf53e67d7a2c0b59406",
          "url": "../../components/update-status/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"solution_name": "{{inputs.parameters.solution_name}}",
          "status": "Solver running"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: update-status-3
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.12' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def update_status(solution_name: str = "", status: str = "", depends_on: Artifact = None):
            import requests

            url = f"http://os4ml-jobmanager.os4ml:8000/apis/v1beta1/jobmanager/solution/{solution_name}"

            solution = requests.get(url).json()
            solution["status"] = status

            requests.put(url, json=solution)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - update_status
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, update-status-3, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'solution_name={{inputs.parameters.solution_name}}',
        status=Solver finished, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"solution_name":
          {"type": "STRING"}, "status": {"type": "STRING"}}, "inputArtifacts": {"depends_on":
          {"metadataPath": "/tmp/inputs/depends_on/data", "schemaTitle": "system.Artifact",
          "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters": {},
          "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      - {name: solution_name}
      artifacts:
      - {name: train-random-forest-metrics, path: /tmp/inputs/depends_on/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "d792d5dfcb1825d1452f5784a0d8e487d2c7d8c2d6dd5bf53e67d7a2c0b59406",
          "url": "../../components/update-status/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"solution_name": "{{inputs.parameters.solution_name}}",
          "status": "Solver finished"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: bucket, value: os4ml}
    - {name: file_name, value: titanic.xlsx}
    - {name: solution_name, value: ''}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/titanic-randomforest-pipeline}
  serviceAccountName: pipeline-runner
