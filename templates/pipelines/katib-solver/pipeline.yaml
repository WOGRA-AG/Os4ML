apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: katib-solver-2-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
    pipelines.kubeflow.org/pipeline_compilation_time: '2022-04-22T17:02:55.362351'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "bucket", "type":
      "String"}, {"name": "file_name", "type": "String"}, {"default": "dataset", "name":
      "dataset_file_name", "optional": true, "type": "String"}, {"default": "", "name":
      "pipeline-root"}, {"default": "pipeline/katib-solver", "name": "pipeline-name"}],
      "name": "katib-solver"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
spec:
  entrypoint: katib-solver-2
  templates:
  - name: get-databag
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def get_databag(bucket: str) -> Dataset:
            import json

            def camel_to_snake(camel: str):
                """Converts a camelCase str to a snake_case str."""
                return ''.join('_' + c.lower() if c.isupper() else c for c in camel)

            import requests
            url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/databag/{bucket}'
            response = requests.get(url)
            databag = response.json()
            return json.dumps({
                camel_to_snake(key): value
                for key, value in databag.items()
            })

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - get_databag
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, get-databag, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}}, "inputArtifacts": {}, "outputParameters": {}, "outputArtifacts":
          {"Output": {"schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/Output/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      artifacts:
      - {name: get-databag-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "b87fc21744ddbf14ba76b20c0a4f5ae232a9bdd750d36f783dc928bb5d9a94c6",
          "url": "../../components/get-databag/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: init-databag
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas>=1.4.0' 'xlrd>=2.0.1' 'openpyxl>=3.0.9' 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def init_databag(bucket: str, file_name: str) -> NamedTuple('DatabagInfo',
                                                                    [('databag_type', str),
                                                                     ('dataset', Dataset),
                                                                     ]):
            """
            Inits the databag by specifying its type and creating the dataset.
            If the file is a zip file it should only contain directories in the top level.
            The names of them are used as labels and the files they contain are used as features.
            """
            import pandas as pd
            import pathlib
            import zipfile
            import enum
            import requests
            import tempfile
            from typing import BinaryIO
            from collections.abc import Generator

            class DatabagTypes(str, enum.Enum):
                local_file = 'local_file'
                zip_file = 'zip_file'

            def download_file(url: str, output_file: BinaryIO, chunk_size=128) -> None:
                response = requests.get(url, stream=True)
                for chunk in response.iter_content(chunk_size=chunk_size):
                    output_file.write(chunk)

            def iter_dirs_of_zip_with_labels(zip_file: BinaryIO) -> Generator[tuple[str, str], None, None]:
                with zipfile.ZipFile(zip_file) as root:
                    unpacked_root_dir = next(zipfile.Path(root).iterdir())
                    for label_dir in unpacked_root_dir.iterdir():
                        label = label_dir.name
                        for file in label_dir.iterdir():
                            file_name = file.filename.resolve().relative_to(unpacked_root_dir.parent.filename.resolve())
                            yield str(file_name), label

            data_uri = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/{bucket}/object/{file_name}'
            file_path = pathlib.Path(file_name)
            match file_path.suffix:
                case '.csv':
                    df = pd.read_csv(data_uri)
                    databag_type = DatabagTypes.local_file
                case '.xls' | '.xlsx' | '.xlsm' | '.xlsb' | '.odf' | '.ods':
                    df = pd.read_excel(data_uri, sheet_name=0)
                    databag_type = DatabagTypes.local_file
                case '.zip':
                    with tempfile.NamedTemporaryFile() as tmp_file:
                        download_file(data_uri, tmp_file)
                        df = pd.DataFrame(iter_dirs_of_zip_with_labels(tmp_file), columns=['file', 'label'])
                    databag_type = DatabagTypes.zip_file
                case _:
                    raise NotImplementedError()

            databag_info = NamedTuple('DatabagInfo',
                                      [('databag_type', str),
                                       ('dataset', Dataset),
                                       ])
            return databag_info(databag_type.value, df.to_csv(index=False))

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - init_databag
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, init-databag, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        'file_name={{inputs.parameters.file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {"databag_type": {"type": "STRING", "path": "/tmp/outputs/databag_type/data"}},
          "outputArtifacts": {"dataset": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/dataset/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      artifacts:
      - {name: init-databag-databag_type, path: /tmp/outputs/databag_type/data}
      - {name: init-databag-dataset, path: /tmp/outputs/dataset/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "212b45e37594b2c539544c750458286994c9f8752c60e3839157c5928a530c69",
          "url": "../../components/init-databag/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "file_name": "{{inputs.parameters.file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: katib-solver
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kubeflow-katib>=0.13.0' 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def katib_solver(databag_file: Input[Dataset],
                         dataset_file_name: str,
                         cls_metrics: Output[ClassificationMetrics],
                         metrics: Output[Metrics],
                         parallel_trial_count: int = 3,
                         max_trial_count: int = 12,
                         max_failed_trial_count: int = 1,
                         ):
            from kubeflow.katib import KatibClient
            from kubernetes.client import V1ObjectMeta
            from kubeflow.katib import V1beta1Experiment
            from kubeflow.katib import V1beta1AlgorithmSpec
            from kubeflow.katib import V1beta1ObjectiveSpec
            from kubeflow.katib import V1beta1FeasibleSpace
            from kubeflow.katib import V1beta1ExperimentSpec
            from kubeflow.katib import V1beta1NasConfig
            from kubeflow.katib import V1beta1GraphConfig
            from kubeflow.katib import V1beta1Operation
            from kubeflow.katib import V1beta1ParameterSpec
            from kubeflow.katib import V1beta1TrialTemplate
            from kubeflow.katib import V1beta1TrialParameterSpec

            import json

            with open(databag_file.path) as file:
                databag = json.load(file)

            databag_url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/' \
                          f'databag/{databag["bucket_name"]}'

            dataset_url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/' \
                          f'{databag["bucket_name"]}/object/{dataset_file_name}'

            zip_url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/' \
                      f'{databag["bucket_name"]}/object/{databag["file_name"]}'

            namespace = "os4ml"
            experiment_name = f'katib-solver-{databag["bucket_name"]}'

            metadata = V1ObjectMeta(
                name=experiment_name,
                namespace=namespace
            )

            algorithm_spec = V1beta1AlgorithmSpec(
                algorithm_name="enas",
            )

            objective_spec = V1beta1ObjectiveSpec(
                type="maximize",
                goal=0.99,
                objective_metric_name="Validation-Accuracy",
            )

            nas_config = V1beta1NasConfig(
                graph_config=V1beta1GraphConfig(
                    num_layers=8,
                    input_sizes=[28, 28, 3],
                    output_sizes=[10],
                ),
                operations=[
                    V1beta1Operation(
                        operation_type="convolution",
                        parameters=[
                            V1beta1ParameterSpec(
                                name="filter_size",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["3", "5", "7"]),
                            ),
                            V1beta1ParameterSpec(
                                name="num_filter",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["32", "48", "64", "96", "128"]),
                            ),
                            V1beta1ParameterSpec(
                                name="stride",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["1", "2"]),
                            ),
                        ]
                    ),
                    V1beta1Operation(
                        operation_type="separable_convolution",
                        parameters=[
                            V1beta1ParameterSpec(
                                name="filter_size",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["3", "5", "7"]),
                            ),
                            V1beta1ParameterSpec(
                                name="num_filter",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["32", "48", "64", "96", "128"]),
                            ),
                            V1beta1ParameterSpec(
                                name="stride",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["1", "2"]),
                            ),
                            V1beta1ParameterSpec(
                                name="depth_multiplier",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["1", "2"]),
                            ),
                        ]
                    ),
                    V1beta1Operation(
                        operation_type="depthwise_convolution",
                        parameters=[
                            V1beta1ParameterSpec(
                                name="filter_size",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["3", "5", "7"]),
                            ),
                            V1beta1ParameterSpec(
                                name="stride",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["1", "2"]),
                            ),
                            V1beta1ParameterSpec(
                                name="depth_multiplier",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["1", "2"]),
                            ),
                        ]
                    ),
                    V1beta1Operation(
                        operation_type="reduction",
                        parameters=[
                            V1beta1ParameterSpec(
                                name="reduction_type",
                                parameter_type="categorical",
                                feasible_space=V1beta1FeasibleSpace(list=["max_pooling", "avg_pooling"]),
                            ),
                            V1beta1ParameterSpec(
                                name="pool_size",
                                parameter_type="int",
                                feasible_space=V1beta1FeasibleSpace(min="2", max="3", step="1"),
                            )
                        ]
                    ),
                ]
            )

            trial_spec = {
                "apiVersion": "batch/v1",
                "kind": "Job",
                "spec": {
                    "template": {
                        "metadata": {
                            "annotations": {
                                "sidecar.istio.io/inject": "false"
                            }
                        },
                        "spec": {
                            "containers": [
                                {
                                    "name": "training-container",
                                    "image": "gitlab-registry.wogra.com/developer/wogra/os4ml/"
                                             "enas-trial:509ba445",
                                    "command": [
                                        'python3 ',
                                        '-u ',
                                        'RunTrial.py '
                                        '--architecture="${trialParameters.neuralNetworkArchitecture}" '
                                        '--nn_config="${trialParameters.neuralNetworkConfig}" ',
                                        f'--databag_file={databag_url}',
                                        f'--dataset_file={dataset_url}',
                                        f'--zip_file={zip_url}'
                                    ],
                                    # Training container requires 1 GPU.
                                    "resources": {
                                        "limits": {
                                            "nvidia.com/gpu": 1
                                        }
                                    }
                                },
                            ],
                            "imagePullSecrets": [
                                {
                                    "name": "registry-credentials"
                                }
                            ],
                            "restartPolicy": "Never"
                        }
                    }
                }
            }

            # Template with Trial parameters and Trial spec.
            # Set retain to True to save trial resources after completion.
            trial_template = V1beta1TrialTemplate(
                # retain=True,
                primary_container_name="training-container",
                trial_parameters=[
                    V1beta1TrialParameterSpec(
                        name="neuralNetworkArchitecture",
                        description="NN architecture contains operations ID on each NN layer and skip connections between "
                                    "layers",
                        reference="architecture",
                    ),
                    V1beta1TrialParameterSpec(
                        name="neuralNetworkConfig",
                        description="Configuration contains NN number of layers, input and output sizes, description what each "
                                    "operation ID means",
                        reference="nn_config",
                    ),
                ],
                trial_spec=trial_spec
            )

            # Experiment object.
            experiment = V1beta1Experiment(
                api_version="kubeflow.org/v1beta1",
                kind="Experiment",
                metadata=metadata,
                spec=V1beta1ExperimentSpec(
                    parallel_trial_count=parallel_trial_count,
                    max_trial_count=max_trial_count,
                    max_failed_trial_count=max_failed_trial_count,
                    objective=objective_spec,
                    algorithm=algorithm_spec,
                    nas_config=nas_config,
                    trial_template=trial_template,
                )
            )

            # Create client.
            kclient = KatibClient()

            # Create your Experiment.
            kclient.create_experiment(experiment, namespace=namespace)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - katib_solver
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, katib-solver, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'dataset_file_name={{inputs.parameters.dataset_file_name}}',
        max_failed_trial_count=1, max_trial_count=5, parallel_trial_count=1, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"dataset_file_name":
          {"type": "STRING"}, "max_failed_trial_count": {"type": "INT"}, "max_trial_count":
          {"type": "INT"}, "parallel_trial_count": {"type": "INT"}}, "inputArtifacts":
          {"databag_file": {"metadataPath": "/tmp/inputs/databag_file/data", "schemaTitle":
          "system.Dataset", "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters":
          {}, "outputArtifacts": {"cls_metrics": {"schemaTitle": "system.ClassificationMetrics",
          "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/cls_metrics/data"},
          "metrics": {"schemaTitle": "system.Metrics", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: dataset_file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: get-databag-Output, path: /tmp/inputs/databag_file/data}
    outputs:
      artifacts:
      - {name: katib-solver-cls_metrics, path: /tmp/outputs/cls_metrics/data}
      - {name: katib-solver-metrics, path: /tmp/outputs/metrics/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "97299b5dd452e9ea9173320e02c119b3931fb549db6c35f3e00663905e7bae43",
          "url": "../../components/katib-solver/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_file_name": "{{inputs.parameters.dataset_file_name}}",
          "max_failed_trial_count": "1", "max_trial_count": "5", "parallel_trial_count":
          "1"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: katib-solver-2
    inputs:
      parameters:
      - {name: bucket}
      - {name: dataset_file_name}
      - {name: file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
    dag:
      tasks:
      - name: get-databag
        template: get-databag
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: init-databag
        template: init-databag
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: file_name, value: '{{inputs.parameters.file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: katib-solver
        template: katib-solver
        dependencies: [get-databag]
        arguments:
          parameters:
          - {name: dataset_file_name, value: '{{inputs.parameters.dataset_file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: get-databag-Output, from: '{{tasks.get-databag.outputs.artifacts.get-databag-Output}}'}
      - name: upload-file-to-objectstore
        template: upload-file-to-objectstore
        dependencies: [init-databag]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: dataset_file_name, value: '{{inputs.parameters.dataset_file_name}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: init-databag-dataset, from: '{{tasks.init-databag.outputs.artifacts.init-databag-dataset}}'}
  - name: upload-file-to-objectstore
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def upload_file_to_objectstore(file: Input[Dataset],
                                       bucket: str,
                                       file_name: str):
            import requests
            url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1' \
                  f'/objectstore/{bucket}/object/{file_name}'
            with open(file.path, 'rb') as payload:
                requests.put(url, data=payload)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - upload_file_to_objectstore
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, upload-file-to-objectstore,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'bucket={{inputs.parameters.bucket}}',
        'file_name={{inputs.parameters.dataset_file_name}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.10.2-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"bucket": {"type":
          "STRING"}, "file_name": {"type": "STRING"}}, "inputArtifacts": {"file":
          {"metadataPath": "/tmp/inputs/file/data", "schemaTitle": "system.Dataset",
          "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters": {},
          "outputArtifacts": {}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.10.2-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: bucket}
      - {name: dataset_file_name}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: init-databag-dataset, path: /tmp/inputs/file/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{"digest": "6c3cab35dc7025fc6985c5453f99f8e7a211aaeae498f781c1bd7ba6c7faa491",
          "url": "../../components/upload-to-objectstore/component.yaml"}'
        pipelines.kubeflow.org/arguments.parameters: '{"bucket": "{{inputs.parameters.bucket}}",
          "file_name": "{{inputs.parameters.dataset_file_name}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: bucket}
    - {name: file_name}
    - {name: dataset_file_name, value: dataset}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/katib-solver}
  serviceAccountName: pipeline-runner
