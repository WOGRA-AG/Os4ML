name: Ludwig solver
inputs:
- {name: dataset_file, type: Dataset}
- {name: databag_file, type: Dataset}
- {name: batch_size, type: Integer, default: '8', optional: true}
- {name: epochs, type: Integer, default: '50', optional: true}
- {name: early_stop, type: Integer, default: '10', optional: true}
outputs:
- {name: cls_metrics, type: ClassificationMetrics}
- {name: metrics, type: Metrics}
- {name: Output, type: Artifact}
implementation:
  container:
    image: gitlab-registry.wogra.com/developer/wogra/os4ml/template-ludwig:164-genimgsol-databag-aus-bildern-erzeugen
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.12' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def ludwig_solver(dataset_file: Input[Dataset],
                        databag_file: Input[Dataset],
                        cls_metrics: Output[ClassificationMetrics],
                        metrics: Output[Metrics],
                        batch_size: int = 8,
                        epochs: int = 50,
                        early_stop: int = 10) -> Artifact:
          from ludwig.api import LudwigModel
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import confusion_matrix
          from typing import List, BinaryIO
          import dataclasses
          import pandas as pd
          import enum
          import json
          import logging
          import requests
          import zipfile
          import pathlib

          class ColumnDataType(str, enum.Enum):
              NUMERICAL = 'numerical'
              DATE = 'date'
              CATEGORY = 'category'
              TEXT = 'text'
              IMAGE = 'image'

          class ColumnUsage(str, enum.Enum):
              LABEL = 'label'
              FEATURE = 'feature'

          class DatabagTypes(str, enum.Enum):
              local_file = 'local_file'
              zip_file = 'zip_file'

          @dataclasses.dataclass
          class Column:
              name: str
              type: str
              usage: str
              num_entries: int

          def build_model_definition(columns: List[Column]):
              definition = {
                  "input_features": [],
                  "output_features": [],
                  "training": {
                      "batch_size": batch_size,
                      "epochs": epochs,
                      "early_stop": early_stop,
                  }
              }
              feature_descriptions = (
                  create_feature_description(column)
                  for column in columns
                  if column.usage == ColumnUsage.FEATURE
              )
              label_descriptions = (
                  create_label_description(column)
                  for column in columns
                  if column.usage == ColumnUsage.LABEL
              )
              definition['input_features'].extend(feature_descriptions)
              definition['output_features'].extend(label_descriptions)
              return definition

          def create_feature_description(feature: Column) -> dict:
              feature_desc = {
                  'name': feature.name,
                  'type': feature.type,
              }
              if feature.type == ColumnDataType.NUMERICAL:
                  feature_desc['preprocessing'] = {
                      'fill_value': 0
                  }
              return feature_desc

          def create_label_description(label: Column) -> dict:
              label_type = ColumnDataType.NUMERICAL \
                  if label.type == ColumnDataType.NUMERICAL \
                  else ColumnDataType.CATEGORY
              return {
                  'name': label.name,
                  'type': label_type,
              }

          def download_file(url: str, output_file: BinaryIO, chunk_size=128) -> None:
              response = requests.get(url, stream=True)
              for chunk in response.iter_content(chunk_size=chunk_size):
                  output_file.write(chunk)

          def path_to_absolute(rel_path: str):
              rel = pathlib.Path(rel_path)
              return str(rel.resolve())

          def download_zip(output):
              bucket = databag['bucket_name']
              file_name = databag['file_name']
              url = f'http://os4ml-objectstore-manager.os4ml:8000/apis/v1beta1/objectstore/{bucket}/object/{file_name}'
              with open(output, 'wb') as output_file:
                  download_file(url, output_file)

          with open(databag_file.path) as file:
              databag = json.load(file)

          columns = [
              Column(**column_dict)
              for column_dict in databag['columns']
          ]

          model_definition = build_model_definition(columns)
          model = LudwigModel(model_definition, logging_level=logging.INFO)

          with open(dataset_file.path, 'r') as input_file:
              dataset = pd.read_csv(input_file)

          if databag['dataset_type'] == DatabagTypes.zip_file:
              zip_file = 'dataset.zip'
              download_zip(zip_file)
              with zipfile.ZipFile(zip_file) as ds_zip:
                  ds_zip.extractall()
              dataset['file'] = dataset['file'].map(path_to_absolute)

          df_tmp, df_test = train_test_split(dataset, test_size=0.1, random_state=42)
          df_train, df_validate = train_test_split(df_tmp, test_size=0.1,
                                                   random_state=42)
          label = model_definition['output_features'][0]['name']
          categories = dataset[label].unique().astype(str)

          model.train(df_train, df_validate, df_test)
          stats, pred, _ = model.evaluate(df_test, collect_predictions=True)

          accuracy = float(stats[label]['accuracy'])

          prediction_key = next(iter(pred))
          y_pred = pred[prediction_key]
          y_true = df_test[label].astype(str)
          conf_matrix = confusion_matrix(y_true, y_pred, labels=categories).tolist()

          metrics.log_metric('accuracy', accuracy)
          cls_metrics.log_confusion_matrix(categories, conf_matrix)

          return metrics

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - ludwig_solver
